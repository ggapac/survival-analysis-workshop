---
title: "Predictive Modelling"
output:
  html_document:
    df_print: paged
---

# Survival Analysis: Predictive Modelling

```{r}
library("randomForestSRC")
library("survival")
library("flexsurvreg")
source("../src/util.R")

dat <- read.csv("../data/veteran_preprocessed.csv")
dat$celltype <- as.factor(dat$celltype)
```

Survival models prove to be a useful tool in many practical scenarios. Doctors can use their estimates to inform the patient about the likely outcomes of their disease or to make informed decisions about the patient’s therapy, the models can help scientists design randomized clinical studies, and the prediction estimates can assist the authorities with healthcare planning and resource distribution. It is therefore of great importance to pay special attention to model evaluation, which, as we present in this notebook, is in this case not so trivial due to the distinguishing feature of survival data – censoring.

## Measures of prediction error

Choice of a suitable measure for prediction error is application-specific. We can be interested in group- or individual-level predictions and evaluate overall performance through time or only focus on predictions at a particular time point, to name a few.

Let's get back to our Veterans example and see how it works in practice.

First we preprocess the dataset: we standardize numeric variables to avoid convergence issues and we apply one-hot encoding to our categorical variable `celltype`.

```{r preprocessing}
# note: make it stratified, or pick the right seed :D
set.seed(8)

train_size <- 0.7
dat$id <- 1:nrow(dat)
train_ix <- sample(1:nrow(dat), round(train_size * nrow(dat)))
train_dat <- dat[train_ix,]
test_dat <- dat[-train_ix,]

# standardize numeric
numeric_cols <- c("karno", "diagtime", "age")
for (col in numeric_cols) {
    col_mean <- mean(train_dat[[col]])
    col_sd <- sd(train_dat[[col]])
    
    train_dat[[col]] <- c(scale(train_dat[[col]]))
    test_dat[[col]] <- c(scale(test_dat[[col]],
                               center = col_mean,
                               scale = col_sd))
}

# dummy variables
cell_type_dummy_train <- model.matrix(~ celltype, data = train_dat)
train_dat$celltype <- NULL
train_dat <- cbind(train_dat, cell_type_dummy_train[, -1])

cell_type_dummy_test <- model.matrix(~ celltype, data = test_dat)
test_dat$celltype <- NULL
test_dat <- cbind(test_dat, cell_type_dummy_test[, -1])
```

In our experiment we test 4 different models that we mentioned today:

-   Cox PH model
-   Exponential model
-   Weibull model
-   Random survival forests

We use all the features available in the dataset. At this point we do not bother with any feature engineering or feature selection, but it is good to have these steps in mind in our real-life use cases!

```{r experiment_parameters}
features <- c("age", 
              "karno", 
              "diagtime", 
              "prior", 
              "trt", 
              "celltypelarge", 
              "celltypesmallcell", 
              "celltypesquamous")

models <- c("cox", "exp", "weibull", "rfsrc")
```

```{r f_fit_predict}
f_fit <- function(model_type, form, train_dat) {
    
    if (model_type == "cox") {
        coxph(form, data = train_dat)
    } else if (model_type == "exp") {
        flexsurvreg(formula=form, data=train_dat, dist="exp")
    } else if (model_type == "weibull") {
        flexsurvreg(formula=form, data=train_dat, dist="weibull")
    } else if (model_type == "rfsrc") {
        rfsrc.fast(formula=form, data=train_dat, forest=TRUE)
    }
}

f_predict <- function(model_type, model, test_dat) {
    if (model_type == "cox") {
        pred <- survfit(model, newdata=test_dat)
        pred_summary <- summary(pred, times=unique(test_dat$time))
        surv_probs <- pred_summary$surv
    } else if (model_type == "exp" || model_type == "weibull") {
        pred <- predict(model, 
                        newdata=test_dat, 
                        times=unique(test_dat$time), 
                        type="survival")
        surv_probs <- NULL
        for (i in 1:nrow(pred)) {
            surv_probs <- cbind(surv_probs, pred[i, ]$.pred[[1]]$.pred_survival)
        }
        times <- pred[1, ]$.pred[[1]]$.time
        surv_probs <- surv_probs[order(times), ]
    } else if (model_type == "rfsrc") {
        pred <- predictSurvProb(fitted, newdata=test_dat, times=unique(sort(test_dat$time)))
        t(pred)
    }
}
```

```{r modelling}
form = as.formula(paste0("Surv(time,status)~", paste0(features, collapse="+")))

fitted_models <- list()
model_predictions <- list()
for (modl in models) {
    fitted_models[[modl]] <- f_fit(modl, form, train_dat)
    model_predictions[[modl]] <- f_predict(modl, fitted_models[[modl]], test_dat)
}
```

### Concordance index

Concordance index, also commonly known as the C-statistic or C-index, is a time-independent metric that provides an overall assessment of the model discrimination power. It measures how well the model can separate the individuals and is essentially a ranking measure, since the exact values are not important, it only focuses on the relative ordering. The most common concordance index is also referred to as *Harrell's C-index*, named after the author. It is defined as

$$
C = \frac{\sum_{i,j}I(\tau_i > \tau_j)I(i\text{ has better predictive outcome})\delta_j}{\sum_{i,j}I(\tau_i > \tau_j)\delta_j},
$$

where we consider all comparable pairs of individuals. A pair is considered comparable if the individual with a shorter survival time $\tau_j$ is not censored ($\delta_j \neq 0$). In the numerator we count 1 if our model gave the individual with the longer survival time better predictive outcome (this can be for example lower risk score, higher survival probability, longer survival time). Note that $C=0.5$ indicates no predictive discrimination and $C=1$ means perfect separation.

Harrell's C-index received a lot of criticism, and even the author himself states that it is not a suitable measure for detecting small differences when we want to compare different models.

```{r c_index}

is_concordant <- function(i, j, tmp_time, unique_times, surv_probs) {
    # A pair of individuals is concordant t_i > t_j and i has lower surv. probability.
    
    time_ix <- which(unique_times == tmp_time)
    if (surv_probs[time_ix, i] < surv_probs[time_ix, j]) {
        return(TRUE)
    }
    return(FALSE)
}

my_c_index <- function(test_dat, surv_probs) {
    
    conc_pairs <- 0
    disc_pairs <- 0
    unique_times <- unique(sort(test_dat$time))
    
    for (i in 1:nrow(test_dat)) {
        for (j in 1:nrow(test_dat)) {
            # skip if:
            #    - same patient
            #    - both censored
            #    - one censored but censoring happened before the event
            if ((i == j) || 
                (test_dat$status[i] == 0 && test_dat$status[j] == 0) ||
                (test_dat$status[i] == 0 &&
                 test_dat$status[j] == 1 && 
                 test_dat$time[i] < test_dat$time[j]) ||
                (test_dat$status[i] == 1 &&
                 test_dat$status[j] == 0 && 
                 test_dat$time[j] < test_dat$time[i])) next
            
            # when both experience the event or one censored but censoring
            # happened after the event
            conc <- NULL
            if (test_dat$time[i] < test_dat$time[j]) {
                conc <- is_concordant(i, 
                                      j, 
                                      test_dat$time[i],
                                      unique_times,
                                      surv_probs)
                
                conc_pairs <- conc_pairs + conc
                disc_pairs <- disc_pairs + !conc
            }
        }
    }
    
    conc_pairs / (conc_pairs + disc_pairs)
}

c_indexes <- NULL
for (modl in names(model_predictions)) {
    tmp_c_ix <- my_c_index(test_dat, model_predictions[[modl]])
    c_indexes <- rbind(c_indexes,
                       data.frame("model" = modl,
                                  "c_index" = tmp_c_ix))
}
c_indexes
```

### Model evaluation at time point t: IPCW Brier Score

A common scenario is to predict survival status at a pre-defined time point $t$, i.e. if the individual experienced the event by $t$ or if they are still event-free. Probabilistic predictions are evaluated with scoring rules, such as Brier score, log loss etc. In this example we focus on Brier score.

Let $N$ be the number of individuals and let $t$ be the time point of interest, where $y_i(t)$ represents the event status and $p_i(t)$ is the predicted survival probability at time $t$.

Brier score can be thought of as the mean squared error of our prediction. In the absence of censoring we can define Brier score for the $i$-th individual at time point $t$ in the following form:

$$
BS(t) = \frac{1}{N} \sum_{i=1}^{N} (y_i(t) - p_i(t))^2.
$$

Brier score ranges from 0 to 1 and even though it is called a score, our aim is to minimize it.

However, if an individual is censored, it is not possible to evaluate the loss function for their predicted probability since their actual status is unknown. It turns out that instead of ignoring them, we can still use their data in our model evaluation indirectly by applying **inverse probability weighting for censoring** (IPCW).

Let $\tau_i$ be the event time for individual $i$. Their inverse probability of censoring weight is defined as

$$
w_i = \begin{cases}
1/G(t|\mathbf{x}_i)), &\text{if $t < \tau_i$}\\
1/G(\tau_i|\mathbf{x}_i), &\text{if $t\geq \tau_i$ and $\delta_i = 1$}\\
0, &\text{otherwise,}
\end{cases}
$$

where $G(c|\mathbf{x}_i) = P(C > c | \mathbf{x}_i)$ is a conditional censoring function, most often the Kaplan-Meier or Cox PH estimate of the censoring distribution. Each individual falls into one of the three categories: 1) event-free at $t$; 2) event occurred before $t$; 3) censoring occurred before $t$. By using weights $w_i$ we can make sure every individual contributes to the final score, even the ones from category 3). Because their status at time point $t$ is unknown, their weight is 0, however, their contribution is implicit, as it is considered in the censoring distribution.

Now we can define the IPCW Brier score definition by taking into account the weights: $$
BS_{IPCW}(t) = \frac{1}{N}\sum_{i=1}^{N} w_i(y_i(t) - p_i(t))^2.
$$

```{r ipcw_brier}
get_individual_ipcw_weights <- function(test_dat, weights, tp) {
    # Get weights for each individual.
    
    w_i <- rep(NA, nrow(test_dat))
    
    for (i in 1:nrow(test_dat)) {
        
        # If censored before time point, the weight is 0
        if (test_dat$time[i] < tp && test_dat$status[i] == 0) {
            w_i[i] <- 0
        } else {
            w_denom <- weights$w[weights$id == test_dat$id[i]]
            w_i[i] <- 1 / w_denom
        }
    }
    w_i
}

get_censoring_probs <- function(train_dat, test_dat, tp) {
    # Use Kaplan-Meier to model the censoring distribution. 
    
    train_dat <- train_dat[order(train_dat$time), ]
    test_dat <- test_dat[order(test_dat$time, -test_dat$status), ]
    
    cens_probs_df <- NULL
    
    form <- as.formula(paste0("Surv(time, 1-status)~1"))
    cens_fit <- survfit(form, dat=train_dat)
    tmp <- summary(cens_fit, times=test_dat$time)

    cens_probs_df <- data.frame("id" = test_dat$id,
                                "w" = tmp$surv,
                                "times" = test_dat$time)
    
    cens_probs_df
}

get_ipcwBrier <- function(y, pred, w) {
    # Calculate IPCW Brier.
    
    bs <- w * (pred - y)^2
    bs
}

```

```{r}
times <- c(15, 30, 45, 59, 80, 118, 139, 153, 182, 201, 231, 278, 340)


b_scores <- NULL
for (modl in models) {
    for (tp in times) {
        
        # get individual's status at time tp
        # tmp_train dat is used when calculating the weights, tmp_test_dat for prediction
        tmp_train_dat <- train_dat
        tmp_train_dat <- tmp_train_dat[order(tmp_train_dat$time), ]
        tmp_train_dat$status[tmp_train_dat$status == 1 & tmp_train_dat$time > tp] <- 0
        tmp_test_dat <- test_dat
        tmp_test_dat <- tmp_test_dat[order(tmp_test_dat$time), ]
        tmp_test_dat$status[tmp_test_dat$status == 1 & tmp_test_dat$time > tp] <- 0
        
        # get weights
        cens_probs <- get_censoring_probs(train_dat, tmp_test_dat, tp)
        ipcw_weights <- get_individual_ipcw_weights(tmp_test_dat, cens_probs, tp)
        
        time_ix <- which(sort(unique(test_dat$time)) == tp)
        tmp_brier <- get_ipcwBrier(tmp_test_dat$status, 
                                   1-model_predictions[[modl]][time_ix, ], # 1 - survival probability, because status=1 indicates event happening
                                   ipcw_weights)
        
        # use bootstrap to calculate uncertainty
        boot_distr <- bootstrap(tmp_brier)
        ci <- boot_perc_ci(boot_distr)
        
        b_scores <- rbind(b_scores,
                          data.frame("model" = modl,
                                     "t" = tp,
                                     "brier" = mean(tmp_brier),
                                     "ci_low" = ci[["CIlow"]],
                                     "ci_upp" = ci[["CIupp"]]))
    }
}

```

```{r}
ggplot(b_scores, aes(x=t, y=brier, color=model)) + 
    geom_point() + 
    geom_line() + 
    geom_errorbar(aes(ymin = ci_low, ymax = ci_upp), width=5) +
    ylab("IPCW Brier Score") +
    theme_bw()
```

### Other metrics to consider

There are many other metrics that we find in related work. Note that the IPCW methodology is not limited to the Brier score, it can be utilized with other metrics as well.

#### Log Loss at time t

Similar to Brier Score, we can also use log loss for evaluating our models at time $t$:

$$
LL(t) = - \frac{1}{N} \sum_{i=1}^{N} \big(y_i(t) \log{p_i(t)} + (1 - y_i(t))\log{(1-p_i(t))}\big).
$$

#### Integrated Brier Score

If we do not want to restrict ourselves to a single time point we can calculate the Integrated Brier Score (IBS), which provides an overall calculation of the model performance at all available times:

$$
    IBS(t_{max}) = \frac{1}{t_{max}} \int_{0}^{t_{max}} BS(t) dt.
    $$

#### Time-dependent AUC

The receiver operating characteristic (ROC) curve and the area under the ROC curve (AUC) can be extended to survival data by defining sensitivity and specificity as time-dependent measures by classifying positive and negative examples with respect to a certain time. AUC is defined as

$$
AUC(t) = \frac{\sum_{i=1}^{N}\sum_{j=1}^{N}I(\tau_i \leq t, \tau_j > t)I(p_i(t) \geq p_j(t))}{(\sum_{i=1}^{N}I(\tau_i \leq t))(\sum_{i=1}^{N} I(\tau_j > t))},
$$

where $\tau_i$ marks the event time for the $i$-th individual, and $I(\cdot)$ is the indicator function. Similar to the concordance index, AUC is a discrimination measure. While the concordance index measures how well the model can order individuals according to their survival times in general, time-dependent AUC measures how well the model can order them at time $t$.
